────────────────────────────────────────────────────────────────────────────
🧱 MIXED-FORMAT INGESTION PIPELINE: End-to-End Chunking for RAG
────────────────────────────────────────────────────────────────────────────

This shows how clean-gpt-json ingests any combo of `.pdf`, `.md`, `.json`, `.html`, or `.epub`,
cleans and chunks them, and produces per-document JSONs ready for embedding in RAG systems.

─────────────────────────────
1. 📂 DROP FILES INTO INGESTION SOURCE
─────────────────────────────
Put any supported document types into:
→ `ingestion_source/`

Example:
  ingestion_source/
  ├── pinecone.json
  ├── datasheet.pdf
  ├── system_manual.md
  ├── support_pages.epub

─────────────────────────────
2. 🧠 SMART INGEST (make run → smart_ingest.py)
─────────────────────────────
File types auto-detected and parsed:
- PDF → sentence-chunked pages
- MD → paragraph windows
- JSON → flattened crawler format
- HTML → stripped DOM
- EPUB → parsed chapter DOMs

Output:
→ `full/unified.json`

──────────────────────────────────────────────────────────────
ASCII: Raw Input → Unified Chunk List
──────────────────────────────────────────────────────────────

ingestion_source/               full/unified.json
┌────────────┐                  ┌────────────────────────────────┐
│ pinecone.json│ → parse →     │ [                              │
│ datasheet.pdf│ → extract →   │   { "source": "pinecone_1",... │
│ system.md    │ → window  →   │   { "source": "datasheet_3",...│
│ support.epub │ → strip   →   │   { "source": "support_8",...  │
└────────────┘                  │   ...                          │
                               │ ]                              │
                               └────────────────────────────────┘

─────────────────────────────
3. 🧹 CLEAN TEXT
─────────────────────────────
Remove markdown images, HTML tags, empty/junk chunks.

Script:
  `scripts/clean_json_chunks.py`

Output:
→ `full/unified-clean.json`

─────────────────────────────
4. ✂️ SPLIT BY SOURCE (group by metadata.doc_id)
─────────────────────────────
Chunks are grouped by `doc_id` and saved individually.

Script:
  `scripts/split_ready_for_customgpt.py`

Output:
→ `split/{doc_id}.json`

──────────────────────────────────────────────────────────────
ASCII: Unified → Split Output
──────────────────────────────────────────────────────────────

full/unified-clean.json         split/
┌──────────────────────────┐    ┌────────────────────────────┐
│ [                        │    │ pinecone.json              │
│  {metadata: {doc_id: X}},│ → │ datasheet.json             │
│  {metadata: {doc_id: Y}},│    │ support_pages.json         │
│  ...                     │    └────────────────────────────┘
└──────────────────────────┘

─────────────────────────────
5. 🧠 INJECT TITLES (optional)
─────────────────────────────
Infer `metadata.title` from metadata.url or source fallback.

Script:
  `scripts/inject_titles_from_source.py`

─────────────────────────────
6. ✅ VALIDATE STRUCTURE
─────────────────────────────
Ensure chunk structure is valid JSON and properly formed.

Script:
  `scripts/validate_json_output.py`

─────────────────────────────
7. 🎯 READY FOR EMBEDDING
─────────────────────────────
Each JSON in `/split/` is ready for vector embedding or LLM-based RAG querying.

You now have per-document, clean, semantically chunked JSONs, tagged with origin + title.
